{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, ExtraTreesClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import pickle\n",
        "\n"
      ],
      "metadata": {
        "id": "GcMnsn1bgl99"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset (replace 'your_dataset.csv' with the actual dataset file)\n",
        "data = pd.read_csv('diabetes.csv')"
      ],
      "metadata": {
        "id": "11SQ_2lV8Lhv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features (X) and target (y)\n",
        "X = data[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']]\n",
        "y = data['Outcome']  # Assuming 'Outcome' is the target variable (0 or 1)"
      ],
      "metadata": {
        "id": "l-RSyjgr8QCM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Oversample the minority class to create a balanced dataset\n",
        "oversampler = RandomOverSampler(sampling_strategy='minority', random_state=42)\n",
        "X_resampled, y_resampled = oversampler.fit_resample(X, y)\n"
      ],
      "metadata": {
        "id": "lqZ6-amG8W0s"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Selection using SelectKBest and chi-squared\n",
        "selector = SelectKBest(chi2, k=5)  # Adjust the number of features (k) as needed\n",
        "X_resampled = selector.fit_transform(X_resampled, y_resampled)"
      ],
      "metadata": {
        "id": "g8Xn3acP8Xwt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the resampled data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "le-fhCe48jWU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize features (optional but often beneficial)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define machine learning models\n",
        "models = [\n",
        "    ('Random Forest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('Extra Trees', ExtraTreesClassifier(n_estimators=100, random_state=42)),\n",
        "    ('XGBoost', XGBClassifier(n_estimators=100, random_state=42)),\n",
        "    ('Gradient Boosting', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
        "    ('SVM', SVC(kernel='linear', C=1, probability=True, random_state=42)),\n",
        "    ('KNN', KNeighborsClassifier(n_neighbors=5)),\n",
        "    ('Naive Bayes', GaussianNB())\n",
        "]\n"
      ],
      "metadata": {
        "id": "b00HPIyC8ks1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning using GridSearchCV for Extra Trees\n",
        "best_models = {}\n",
        "for name, model in models:\n",
        "    param_grid = {}  # Define hyperparameter grid for each model\n",
        "    if name == 'SVM':\n",
        "        param_grid = {'C': [0.1, 1, 10]}\n",
        "    elif name == 'KNN':\n",
        "        param_grid = {'n_neighbors': [3, 5, 7]}\n",
        "    elif name == 'Extra Trees':  # Include hyperparameters for Extra Trees\n",
        "        param_grid = {'n_estimators': [50, 100, 200]}\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), scoring='accuracy', n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_models[name] = grid_search.best_estimator_\n",
        "\n",
        "# Weighted Ensemble of Base Models\n",
        "ensemble_model = VotingClassifier(estimators=[\n",
        "    ('Random Forest', best_models['Random Forest']),\n",
        "    ('Extra Trees', best_models['Extra Trees']),\n",
        "    ('XGBoost', best_models['XGBoost']),\n",
        "    ('Gradient Boosting', best_models['Gradient Boosting']),\n",
        "    ('SVM', best_models['SVM']),\n",
        "    ('KNN', best_models['KNN']),\n",
        "    ('Naive Bayes', best_models['Naive Bayes'])\n",
        "], voting='soft', weights=[2, 1, 2, 2, 1, 1, 1])"
      ],
      "metadata": {
        "id": "tMWJB_3S87R7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the ensemble model using cross-validation\n",
        "cv_scores = cross_val_score(ensemble_model, X_train, y_train, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42))\n",
        "print(f'Ensemble Model: Cross-Validation Accuracy: {np.mean(cv_scores):.2f} (+/- {np.std(cv_scores):.2f})')\n",
        "\n",
        "# Fit the ensemble model on the entire training dataset\n",
        "ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ensemble_model.predict(X_test)\n",
        "\n",
        "# Evaluate the ensemble model on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Ensemble Model Test Accuracy: {test_accuracy:.2f}')\n",
        "\n",
        "# Generate a classification report for detailed metrics\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4WEYe8K9ALT",
        "outputId": "3141e216-ee90-4087-b57f-7777630e663b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Model: Cross-Validation Accuracy: 0.82 (+/- 0.03)\n",
            "Ensemble Model Test Accuracy: 0.82\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.77      0.81        99\n",
            "           1       0.79      0.88      0.84       101\n",
            "\n",
            "    accuracy                           0.82       200\n",
            "   macro avg       0.83      0.82      0.82       200\n",
            "weighted avg       0.83      0.82      0.82       200\n",
            "\n"
          ]
        }
      ]
    }
  ]
}